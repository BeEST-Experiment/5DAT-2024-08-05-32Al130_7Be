{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Find optimized shaping by grid search\n",
    "\n",
    "Loads a small amount of data for the day/chunk and then performs a grid search\n",
    "to find the optimal shaping parameters. This includes:\n",
    "- The shaping times (rise, flat)\n",
    "- The fast shaping time (rise)\n",
    "The grid search produces a dataframe of data which then needs to be substrate corrected.\n",
    "Finally, a gaussian width of the laser data is reflective of the resolution.\n",
    "\"\"\"\n",
    "\n",
    "import click\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# from cryoant.daq.xia.listmode import find_optimum_processing\n",
    "from cryoant.daq.xia.listmode import load_and_process\n",
    "from beest.laser import correct_substrate_heating\n",
    "import matplotlib.pyplot as plt\n",
    "import cryoant as ct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "import gc\n",
    "\n",
    "\n",
    "class ScatterHistogramPlot:\n",
    "    def __init__(self, x, y, name, settings, axis=\"x\"):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.name = name\n",
    "        self.settings = settings\n",
    "        self.axis = axis\n",
    "\n",
    "    def plot(self):\n",
    "        if self.axis == \"x\":\n",
    "            fig, (ax_scatter, ax_hist) = plt.subplots(\n",
    "                2,\n",
    "                1,\n",
    "                sharex=True,\n",
    "                gridspec_kw={\"height_ratios\": [3, 1]},\n",
    "                figsize=(5, 5),\n",
    "                dpi=500,\n",
    "                constrained_layout=True,\n",
    "            )\n",
    "        else:\n",
    "            fig, (ax_hist, ax_scatter) = plt.subplots(\n",
    "                1,\n",
    "                2,\n",
    "                sharey=True,\n",
    "                gridspec_kw={\"width_ratios\": [1, 3]},\n",
    "                figsize=(5, 5),\n",
    "                dpi=500,\n",
    "                constrained_layout=True,\n",
    "            )\n",
    "\n",
    "        # Scatter plot\n",
    "        ax_scatter.scatter(self.x, self.y, lw=0, s=0.1, alpha=0.2)\n",
    "        ax_scatter.set(**self.settings)\n",
    "\n",
    "        # Histogram\n",
    "        if self.axis == \"x\":\n",
    "            h, e = np.histogram(self.x, bins=1000)\n",
    "            ax_hist.step(e[:-1], h, color=\"blue\")\n",
    "            ax_hist.set(ylabel=\"Count\", yscale=\"log\")\n",
    "        else:\n",
    "            h, e = np.histogram(self.y, bins=1000)\n",
    "            ax_hist.step(h, e[:-1], color=\"blue\")\n",
    "            ax_hist.set(xlabel=\"Count\", xscale=\"log\")\n",
    "\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_processor(file, multithread=True):\n",
    "    # dfi, header, _ = find_optimum_processing(file, multithread=multithread)\n",
    "    dfi, header, _ = load_and_process(file, multithread=multithread)\n",
    "    size = os.path.getsize(file)\n",
    "    print(f\"Size: {size / 1024 / 1024 / 1024} GB\")\n",
    "    if len(dfi) == 0:\n",
    "        print(f\"Skipping {file}\")\n",
    "        return dfi, header, size\n",
    "    dfi[\"ig_laser\"] = np.abs(\n",
    "        (dfi.time % 0.1)\n",
    "        - (dfi.time % 0.1).rolling(min([1000, int(0.01 * len(dfi))])).median()\n",
    "    ) < 0.01 * np.mean(dfi.time % 0.1)\n",
    "    dfi[\"fname\"] = os.path.basename(file)\n",
    "    gc.collect()\n",
    "    return dfi, header, size\n",
    "\n",
    "\n",
    "def process_chunk(files_chunk, no_parallel):\n",
    "    if no_parallel:\n",
    "        return [file_processor(file, multithread=False) for file in files_chunk]\n",
    "    else:\n",
    "        return Parallel(n_jobs=8)(delayed(file_processor)(file) for file in files_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Processor Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Level Debugging\n",
    "\n",
    "Mean of empty slice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Generate scatter from listmode data.\n",
    "\n",
    "date is in YYMMDD format. Gets all files in setup with 20{date} in path.\n",
    "Identify laser by rolling median filter.\n",
    "Scatter laser data for investigation of substrate heating correction.\n",
    "\"\"\"\n",
    "\n",
    "plt.style.use(f\"{list(ct.__path__)[0]}/plot.mpl\")\n",
    "\n",
    "DPI_VIS, DPI_SV = 200, 50\n",
    "DATE = \"240812\"\n",
    "no_parallel = False\n",
    "\n",
    "directory = \"/beest_data/summer2024/Be7_Ta_PR_Mask_listmode_ben/d/\"\n",
    "files = glob.glob(os.path.join(directory, \"*_Al_*/*.bin\"))\n",
    "\n",
    "# Sort files by filesize\n",
    "files.sort(key=os.path.getsize)\n",
    "files = [file for file in files if os.path.getsize(file) > 0]\n",
    "\n",
    "chunk_size = 8  # Define the chunk size for processing\n",
    "df = pd.DataFrame()\n",
    "total, skip = 0, 0\n",
    "files = [file for file in files if f\"20{DATE}\" in os.path.dirname(file)][:4]\n",
    "headers = []\n",
    "for i in range(0, len(files) + chunk_size, chunk_size):\n",
    "    files_chunk = files[i : i + chunk_size]\n",
    "    print(f\"Processing chunk {1 + i // chunk_size}/{-(-len(files)//chunk_size)}\")\n",
    "    results = process_chunk(files_chunk, no_parallel)\n",
    "    for result in results:\n",
    "        if result is None:\n",
    "            skip += 1\n",
    "            continue\n",
    "        dfi, header, size = result\n",
    "        if len(dfi) == 0:\n",
    "            skip += 1\n",
    "            continue\n",
    "        headers.append(header)\n",
    "        df = pd.concat([df, dfi])\n",
    "        del dfi  # Explicitly delete dfi to free memory\n",
    "        total += size\n",
    "    del results  # Discard the processed chunk\n",
    "    gc.collect()  # Trigger garbage collection\n",
    "    print(f\"Chunk {1 + i // chunk_size}/{-(-len(files)//chunk_size)} processed\")\n",
    "print(\n",
    "    f\"Processed {len(files) - skip} files, total size: {total / 1024 / 1024 / 1024} GB\"\n",
    ")\n",
    "\n",
    "testcols = [col.split(\"_\")[1] for col in df.columns if \"h_\" == col[:2]]\n",
    "for col in testcols:\n",
    "    fig = ScatterHistogramPlot(\n",
    "        df[f\"hmV_{col}\"],\n",
    "        df[f\"omV_{col}\"],\n",
    "        f\"Shaping: {col}\",\n",
    "        {\n",
    "            \"xlabel\": f\"hmV_{col}\",\n",
    "            \"ylabel\": f\"omV_{col}\",\n",
    "            \"title\": f\"Shaping: {col}\",\n",
    "        },\n",
    "    ).plot()\n",
    "    fig.savefig(f\"out/shapetest_precorrected_{DATE}_{col}.png\")\n",
    "    df[f\"corrected_hmV_{col}\"] = correct_substrate_heating(\n",
    "        df[f\"hmV_{col}\"], df.ig_laser, df[f\"smV_{col}\"]\n",
    "    )\n",
    "    fig = ScatterHistogramPlot(\n",
    "        df[f\"corrected_hmV_{col}\"],\n",
    "        df[f\"omV_{col}\"],\n",
    "        f\"Shaping: {col}\",\n",
    "        {\n",
    "            \"xlabel\": f\"corrected_hmV_{col}\",\n",
    "            \"ylabel\": f\"omV_{col}\",\n",
    "            \"title\": f\"Shaping: {col}\",\n",
    "        },\n",
    "    ).plot()\n",
    "    fig.savefig(f\"out/shapetest_corrected_{DATE}_{col}.png\")\n",
    "\n",
    "print(\"Saving data to HDF5\")\n",
    "df.to_hdf(f\"out/processed/shapetest_{DATE}.h5\", key=\"data\", mode=\"a\")\n",
    "[\n",
    "    pd.Series(header).to_hdf(\n",
    "        f\"out/processed/shapetest_{DATE}.h5\", key=\"metadata\", mode=\"a\"\n",
    "    )\n",
    "    for header in headers\n",
    "]\n",
    "print(\n",
    "    f\"Data Saved. File size: {os.path.getsize(f'out/processed/shapetest_{DATE}.h5') / 1024 / 1024 / 1024} GB\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcols = [col.split(\"_\")[1] for col in df.columns if \"h_\" == col[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "argmaxFFT, maxFFT, angmaxFFT, gradients = deskewFFT(\n",
    "    pulseV_laser, pulseV_laser_other\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "pulseHeight = pulseV_laser\n",
    "otherHeight = pulseV_laser_other\n",
    "bins = 100000\n",
    "voltageRange = [0.0, 1.0]\n",
    "minFFT = 350\n",
    "gradientRange = (-0.040, 0.040, 0.00001)\n",
    "# fmt: off\n",
    "gradients = np.arange(*gradientRange, dtype=np.float32)\n",
    "maxFFT    = np.empty_like(gradients,  dtype=np.float32)\n",
    "angmaxFFT = np.empty_like(gradients,  dtype=np.float32)\n",
    "argmaxFFT = np.empty_like(gradients,  dtype=np.intc)\n",
    "# fmt: on\n",
    "for i, grad in enumerate(gradients):\n",
    "    h, b = np.histogram(pulseHeight - otherHeight * grad, range=voltageRange, bins=bins)\n",
    "    fft = np.fft.rfft(h - np.mean(h), norm=\"ortho\")\n",
    "    argmaxFFT[i] = minFFT + np.argmax(np.abs(fft))\n",
    "    maxFFT[i] = np.abs(fft)[argmaxFFT[i] - minFFT]\n",
    "    angmaxFFT[i] = np.angle(fft[argmaxFFT[i] - minFFT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beest.laser import deskewFFT\n",
    "\n",
    "\"\"\"correct_substrate_heating(\n",
    "        df[f\"hmV_{col}\"], df.ig_laser, df[f\"smV_{col}\"]\n",
    "    )\n",
    "    \"\"\"\n",
    "height_mV = df[f\"hmV_{col}\"]\n",
    "ig_laser = df.ig_laser\n",
    "sumV = df[f\"smV_{col}\"]\n",
    "deskewParams = {}\n",
    "slice_max = 0.025\n",
    "numLines = 20\n",
    "firstLine = 7\n",
    "pulseV_laser = height_mV[ig_laser].astype(np.float32).to_numpy()\n",
    "sum_heights = sumV[ig_laser].astype(np.float32).to_numpy()\n",
    "pulseV_laser_other = sum_heights - pulseV_laser.astype(np.float32)\n",
    "\n",
    "#: TODO Should deskewFFT take all entries to preserve column shape?\n",
    "argmaxFFT, maxFFT, angmaxFFT, gradients = deskewFFT(pulseV_laser, pulseV_laser_other)\n",
    "#: Angmax may vary from -pi to pi, but its basis is still 2pi\n",
    "offset = angmaxFFT[np.argmax(maxFFT)] / 2 / np.pi\n",
    "\n",
    "sliceEdges = np.arange(\n",
    "    #: Offset is located at the peak heights, slice between peaks with 0.5\n",
    "    (0.5 + offset) / argmaxFFT[np.argmax(maxFFT)],\n",
    "    slice_max,\n",
    "    1 / argmaxFFT[np.argmax(maxFFT)],\n",
    ")\n",
    "\n",
    "corrections = np.zeros(numLines, dtype=float)\n",
    "\n",
    "xmin = 0\n",
    "xmax = 0.05\n",
    "xs = pulseV_laser_other\n",
    "ys = pulseV_laser - pulseV_laser_other * gradients[np.argmax(maxFFT)]\n",
    "for i in range(numLines):\n",
    "    ymin = sliceEdges[firstLine + i]\n",
    "    ymax = sliceEdges[firstLine + i + 1]\n",
    "    cutoutx = (pulseV_laser_other < xmax) & (pulseV_laser_other > xmin)\n",
    "    cutouty = (ymin < ys) & (ymax > ys)\n",
    "    cutout = cutoutx & cutouty\n",
    "    try:\n",
    "        poly = np.poly1d(np.polyfit(xs[cutout], ys[cutout], 1))\n",
    "    except TypeError:\n",
    "        continue\n",
    "    corrections[i] = poly[1]\n",
    "avg_correction = np.average(corrections)\n",
    "true_grad = gradients[np.argmax(maxFFT)] + avg_correction\n",
    "\n",
    "corrected = np.array(height_mV.copy())\n",
    "corrected[ig_laser] = height_mV[ig_laser] - pulseV_laser_other * true_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK This sucks. Do it manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Filtering of Data to find Something that looks decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoant.daq.xia.listmode import (\n",
    "    read_listmode_file_optimized,\n",
    "    get_opt_tpz_for_channel,\n",
    ")\n",
    "\n",
    "DATE = \"240812\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"/beest_data/summer2024/Be7_Ta_PR_Mask_listmode_ben/d/\"\n",
    "files = glob.glob(os.path.join(directory, \"*_Al_*/*.bin\"))\n",
    "files.sort(key=os.path.getsize)\n",
    "files = [file for file in files if os.path.getsize(file) > 0]\n",
    "files = [file for file in files if f\"20{DATE}\" in os.path.dirname(file)]\n",
    "filename = files[0]\n",
    "display(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwdict = json.load(open(f\"kwfile-20{DATE}.json\", \"r\"))\n",
    "display(kwdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwdict[\"debug_plots\"] = False\n",
    "kwdict[\"outdir\"] = \"out/dev\"\n",
    "\n",
    "df, header, opt_tpzs = load_and_process(\n",
    "    filename,\n",
    "    known_tpz=None,\n",
    "    multithread=False,\n",
    "    events_total=-1,\n",
    "    drop_trace=False,\n",
    "    kwdict=kwdict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryoant.daq.xia.listmode import extract_pulse_heights_threaded\n",
    "\n",
    "risetest = [100, 700, 100]\n",
    "flattest = [100, 200, 100]\n",
    "fastrise = [10, 100, 50]\n",
    "testcols = []\n",
    "tests = np.array(\n",
    "    np.meshgrid(\n",
    "        np.arange(risetest[0], risetest[1], risetest[2]),\n",
    "        np.arange(flattest[0], flattest[1], flattest[2]),\n",
    "        np.arange(fastrise[0], fastrise[1], fastrise[2]),\n",
    "    )\n",
    ").T.reshape(-1, 3)\n",
    "\n",
    "\n",
    "def tester(params):\n",
    "    rise, flat, fast = params\n",
    "    testcol = f\"r{rise}f{flat}s{fast}\"\n",
    "    heights, fast_amps = extract_pulse_heights_threaded(\n",
    "        df,\n",
    "        opt_tpzs,\n",
    "        rise=rise,\n",
    "        flat=flat,\n",
    "        fall=rise,\n",
    "        fast_rise=fast,\n",
    "        fast_fall=fast,\n",
    "    )\n",
    "    return testcol, heights, fast_amps\n",
    "\n",
    "\n",
    "tester(tests[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for file in files:\n",
    "#     _, df = read_listmode_file_optimized(\n",
    "#                     file, events_total=1e4\n",
    "#                 )\n",
    "#     dfs.append(df)\n",
    "# df = pd.concat(dfs)\n",
    "# del dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
